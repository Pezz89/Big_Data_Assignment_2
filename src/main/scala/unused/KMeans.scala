package ClusterSOData

import org.apache.spark.SparkContext 
import org.apache.spark.SparkContext._ 
import org.apache.spark._

object KMeans {
   /**
    * Run KMeans clustering on an input RDD vector
   */
  def run(
    //data: RDD[Vector]
  ) 
  {
    // val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
    // counts.saveAsTextFile("output")
  }
}
